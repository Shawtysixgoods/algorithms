# Что такое информация? Путеводитель для новичков

**Главная идея:** информация — это то, что убирает нашу неопределённость о событии или объекте. Вся теория информации строится вокруг того, как измерять, хранить, передавать и защищать эту информацию.

***

## 1. Знакомство с информацией: «Коробка с игрушкой»

Представьте: у вас в руках непрозрачная коробка, и внутри лежит одна из трёх игрушек — мячик, кукла или машинка. Пока вы не откроете коробку, вы не знаете, что там лежит. Это и есть **неопределённость**. Когда вы открываете коробку и видите игрушку, неопределённость исчезает — вы получили **информацию**.

– Чем больше вариантов внутри коробки, тем сильнее ваше любопытство и тем ценнее каждая новая подсказка.  
– Если вариантов два, подсказка «игрушка не мячик» сразу выдаёт ответ. Если вариантов десять — одной подсказки точно не хватит.

***

## 2. Почему информация измеряется в битах

Бит — сокращение от «binary digit» (двоичная цифра). Это единица информации, которую даёт ответ на вопрос «да/нет».  

- Если есть 2 варианта (мячик или кукла), нужны 1 бит: один вопрос «Это мячик?» — и вы либо узнаёте, либо нет.  
- Если 4 варианта, минимальное число вопросов «да/нет» равно 2, потому что за два вопроса можно разделить 4 варианта по веткам решений (2² = 4).

**Формула Хартли** для равновероятных вариантов $$N$$:  
$$
\text{Информация (в битах)} = \log_2 N
$$

Примеры:  
- Монета (орёл/решка): $$ \log_2 2 = 1 $$ бит.  
- Кубик (6 граней): $$ \log_2 6 \approx 2{,}6 $$ бита.  
- Колода из 52 карт: $$ \log_2 52 \approx 5{,}7 $$ бита.  

***

## 3. Дискретный источник информации

### 3.1. Что это значит

- **Дискретный источник** генерирует последовательность символов из конечного списка (алфавита).  
- Примеры алфавитов: буквы русского алфавита (33 символа), грани кубика (6 символов), пиктограммы смайликов.

У каждого символа есть **вероятность** $$p_i$$ — насколько часто он появляется.

### 3.2. Примеры моделей

1. **i.i.d. (независимые и одинаково распределённые)**  
   Каждый символ появляется независимо от других с одной и той же вероятностью.  
   *Пример:* многократное подбрасывание честной монеты.

2. **Марковский источник**  
   Вероятность следующего символа зависит от предыдущего.  
   *Пример:* в тексте после «К» чаще идёт «О», а не «Я».

3. **Скрытые модели**  
   Более сложные схемы, где внутреннее состояние источника скрыто, а видно только выходной символ. Используются в речи и биоинформатике.

***

## 4. Энтропия: среднее количество информации

В реальности символы не равновероятны. Чтобы учитывать это, вводится **энтропия Шеннона**:
$$
H = -\sum_{i=1}^{m} p_i \log_2 p_i,
$$
где $$p_i$$ — вероятность $$i$$-го символа. Энтропия показывает, сколько бит нам нужно в среднем, чтобы закодировать один символ этого источника.

- Если символы равновероятны, $$H = \log_2 m$$.  
- Если один символ встречается почти всегда ($$p\approx1$$), $$H\approx0$$.

**Жизненный пример:**  
В русском языке буква «О» встречается гораздо чаще, чем «Й». Это снижает энтропию текста до примерно 4 бит на букву, хотя максимально для 33 букв было бы $$\log_2 33 \approx 5{,}04$$ бита.

***

## 5. Избыточность и её роль

**Избыточность** показывает, какая часть информации предсказуема и не несёт «нового» знания:
$$
r = 1 - \frac{H}{\log_2 m}.
$$
- В русском $$r\approx0{,}7$$ — 70% текста можно предсказать.  
- В случайном тексте без повторений $$r=0$$.

**Зачем нужна избыточность?**  
Она даёт возможность **сжимать** данные: алгоритмы убирают предсказуемые или повторяющиеся части и уменьшают объём без потери смысла.

***

## 6. Кодирование: от Хаффмана до арифметики

### 6.1. Код Хаффмана

1. Сортируем символы по вероятности.  
2. Последовательно объединяем два наименее вероятных в одну «супербукву».  
3. Строим двоичное дерево: короткие коды у частых символов, длинные — у редких.

Результат: средняя длина кода почти равна энтропии источника.

### 6.2. Арифметическое кодирование

Представляет всё сообщение одним дробным числом в [0,1). Очень эффективно приближает энтропию, но чуть сложнее в реализации.

***

## 7. Передача информации и защита от ошибок

### 7.1. Шумные каналы и пропускная способность

В любой линии связи (Wi-Fi, кабель, оптический волоконный канал) возможны помехи. Клод Шеннон доказал, что для любого канала существует максимальная скорость передачи (бит/с), при которой можно добиться сколь угодно малой вероятности ошибки.

### 7.2. Коды исправления ошибок

- **Коды с добавлением избыточных битов**, например коды Хэмминга.  
- **Блочные** (Reed–Solomon) и **сверточные** коды для передачи по спутникам, в мобильных сетях.

Эти алгоритмы добавляют контрольные биты, позволяют обнаруживать и исправлять ошибки на приёме.

***

## 8. Наглядная схема работы источника и канала

```
[Источник] --(кодирование)--> [Канал с шумом] --(декодирование)--> [Приёмник]
     |                                           ^
     |-----------(контроль ошибок и коррекция)--|
```

1. Источник выдаёт символы.  
2. Кодировщик упаковывает их в двоичные строки.  
3. По каналу (с шумом) данные доходят с искажениями.  
4. Декодировщик проверяет контрольные биты, исправляет ошибки и восстанавливает символы.

***

## 9. Практические упражнения

1. **Биты от монет:** сколько бит информации даёт подбрасывание трёх честных монет?  
2. **Энтропия простого источника:** вычислите для вероятностей $$\{0{,}5, 0{,}3, 0{,}15, 0{,}05\}$$.  
3. **Избыточность:** найдите для равновероятного алфавита из 5 символов.  
4. **Код на Nim:** реализуйте код Хаффмана для четырёх символов с заданными вероятностями, посчитайте среднюю длину и сравните с энтропией.

***

## 10. Совет для изучения и дальнейшие шаги

- Всегда **сопоставляйте формулы с примерами**: от простых монет и карт до кода текста.  
- **Пишите код**: кодируйте и декодируйте сообщения, экспериментируйте с разными алгоритмами.  
- **Разбирайтесь в моделях**: i.i.d., марковские и скрытые источники — каждый подходит для разных задач.  
- **Ищите литературу**: классика Шеннона и современные обзоры в области сжатия и коррекции ошибок.

Понимание теории информации даст прочный фундамент для работы с данными, сетями, безопасностью и анализом — а это важно для любого программиста, в том числе на Nim. Удачи в изучении!
